{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '15'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '12'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import warnings\n",
    "import copy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим всех юзеров в 1 датафрейм, для создания фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = pd.read_csv('../data/train.csv')\n",
    "\n",
    "sample = pd.read_csv('../data/sample_submit_naive.csv').drop('predict', axis=1)\n",
    "sample['target'] = -1\n",
    "\n",
    "main = pd.concat([main, sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем дополнительные файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = pd.read_csv('../data/clients.csv')\n",
    "report_dates = pd.read_csv('../data/report_dates.csv', parse_dates=['report_dt'])\n",
    "\n",
    "transactions = pd.read_csv('../data/transactions.csv', parse_dates=['transaction_dttm'])\n",
    "transactions = transactions.sort_values('transaction_dttm').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = copy.deepcopy(transactions)\n",
    "df['mcc_code'] = df['mcc_code'].astype(str)\n",
    "user_docs = df.groupby('user_id')['mcc_code'].apply(' '.join).reset_index()\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'\\b\\d+\\b')\n",
    "tfidf_matrix = vectorizer.fit_transform(user_docs['mcc_code'])\n",
    "tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=user_docs['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим информацию о клиенте, а также закодируем employee_count_nm\n",
    "\n",
    "main = main.merge(clients, how='left', on='user_id')\n",
    "main['employee_count_nm'] = LabelEncoder().fit_transform(main['employee_count_nm'].fillna('unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = main.merge(tfidf, how='left', on='user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для категорий, которые встречались больше 10 раз агрегируем информацию о транзакциях по пользователю и коду MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_codes = transactions['mcc_code'].value_counts()\n",
    "good_codes = good_codes[good_codes >= 10]\n",
    "\n",
    "mcc_info = transactions[transactions.mcc_code.isin(good_codes)].pivot_table(\n",
    "    index = 'user_id',\n",
    "    values=['transaction_amt'],\n",
    "    columns=['mcc_code'],\n",
    "    aggfunc=['count', 'median', 'sum']\n",
    ").fillna(0)\n",
    "mcc_info.columns = ['main_' + '_'.join(map(str, x)) for x in mcc_info.columns]\n",
    "\n",
    "count_cols = [x for x in mcc_info.columns if 'count' in x]\n",
    "mcc_info['sum'] = mcc_info[count_cols].sum(axis=1)\n",
    "for col in count_cols:\n",
    "    mcc_info[f'{col}_norm'] = mcc_info[col] / mcc_info['sum']\n",
    "mcc_info.drop('sum', axis=1, inplace=True)\n",
    "\n",
    "main = main.merge(mcc_info, how='left', left_on='user_id', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сбор информации о транзакциях в каждой валюте за последние 30 дней, а также за весь промежуток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more = transactions.merge(clients[['user_id', 'report']], how='left', on='user_id')\n",
    "df_more = df_more.merge(report_dates, how='left', on='report')\n",
    "df_more['days_to_report'] = (df_more['report_dt'] - df_more['transaction_dttm']).dt.days\n",
    "\n",
    "\n",
    "for day_diff in [30, 1000]:\n",
    "\n",
    "    # Информация о размерах транзакций в различных валютах\n",
    "    currency_pivot = df_more[df_more['days_to_report'] < day_diff + 100].pivot_table(\n",
    "        index='user_id',\n",
    "        columns='currency_rk',\n",
    "        values='transaction_amt',\n",
    "        aggfunc=['sum', 'mean', 'median', 'count']\n",
    "    ).fillna(0)\n",
    "    currency_pivot.columns = [f'currency_daydiff_{day_diff}_{x[0]}_{x[1]}' for x in currency_pivot.columns]\n",
    "\n",
    "    currency_pivot['sum'] = currency_pivot[[x for x in currency_pivot.columns if 'count' in x]].sum(axis=1)\n",
    "    for x in range(4):\n",
    "        currency_pivot[f'currency_daydiff_{day_diff}_count_{x}_norm'] = currency_pivot[f'currency_daydiff_{day_diff}_count_{x}'] / currency_pivot['sum']\n",
    "    currency_pivot.drop('sum', axis=1, inplace=True)\n",
    "\n",
    "    main = main.merge(currency_pivot, how='left', left_on='user_id', right_index=True)\n",
    "\n",
    "\n",
    "    general_trans_info = df_more[df_more['days_to_report'] < day_diff + 100].groupby('user_id')['transaction_amt'].agg(['sum', 'count', 'median'])\n",
    "    general_trans_info[['sum', 'count']] = general_trans_info[['sum', 'count']].fillna(0)\n",
    "    general_trans_info.columns = [f'general_trans_info_{day_diff}_{x}' for x in general_trans_info]\n",
    "    main = main.merge(general_trans_info, how='left', left_on='user_id', right_index=True)\n",
    "\n",
    "    general_trans_info = df_more[(df_more['days_to_report']<day_diff + 100)&(df_more['transaction_amt']>0)].groupby('user_id')['transaction_amt'].agg(['sum', 'count', 'median'])\n",
    "    general_trans_info[['sum', 'count']] = general_trans_info[['sum', 'count']].fillna(0)\n",
    "    general_trans_info.columns = [f'positive_general_trans_info_{day_diff}_{x}' for x in general_trans_info]\n",
    "    main = main.merge(general_trans_info, how='left', left_on='user_id', right_index=True)\n",
    "\n",
    "    general_trans_info = df_more[(df_more['days_to_report']<day_diff + 100)&(df_more['transaction_amt']<0)].groupby('user_id')['transaction_amt'].agg(['sum', 'count', 'median'])\n",
    "    general_trans_info[['sum', 'count']] = general_trans_info[['sum', 'count']].fillna(0)\n",
    "    general_trans_info.columns = [f'negative_general_trans_info_{day_diff}_{x}' for x in general_trans_info]\n",
    "    main = main.merge(general_trans_info, how='left', left_on='user_id', right_index=True)\n",
    "\n",
    "\n",
    "# Анализируем кол-во транзакций в последние n дней / кол-во транзакций до последних n дней\n",
    "for x in [5, 30]:\n",
    "    prev = df_more[df_more['days_to_report'] > x + 100].groupby('user_id')['report'].agg(['count']).reset_index().rename({'count': f'num_transaction_before_{x}_days'}, axis=1)\n",
    "    last = df_more[df_more['days_to_report'] <= x + 100].groupby('user_id')['report'].agg(['count']).reset_index().rename({'count': f'num_transaction_last_{x}_days'}, axis=1)\n",
    "\n",
    "    main = main.merge(prev, how='left', on='user_id')\n",
    "    main = main.merge(last, how='left', on='user_id')\n",
    "    main[f'num_transaction_last_{x}_days'].fillna(0, inplace=True)\n",
    "    main[f'num_transaction_before_{x}_days'].fillna(0, inplace=True)\n",
    "    main[f'percent_last_{x}'] = main[f'num_transaction_last_{x}_days'] / main[f'num_transaction_before_{x}_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_tr = df_more.loc[df_more['transaction_amt'] < 0].groupby('user_id', as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кол-во уникальных MCC кодов, валют, а также уникальных дней, в которые были транзакции\n",
    "main = main.merge(df_more.groupby('user_id')['days_to_report'].nunique(), how='left', on='user_id').rename({'days_to_report': 'nunique_days'}, axis=1)\n",
    "main = main.merge(df_more.groupby('user_id')['mcc_code'].nunique(), how='left', on='user_id').rename({'mcc_code': 'nunique_mcc_codes'}, axis=1)\n",
    "main = main.merge(df_more.groupby('user_id')['currency_rk'].nunique(), how='left', on='user_id').rename({'currency_rk': 'nunique_currency'}, axis=1)\n",
    "\n",
    "main = main.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Информация о количестве и размере транзакций в разрезе часов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = transactions.copy()\n",
    "tmp['hour'] = tmp['transaction_dttm'].dt.hour\n",
    "pivot_table = tmp.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='hour',\n",
    "    values='transaction_amt',\n",
    "    aggfunc=['count', 'median']\n",
    ").fillna(0)\n",
    "pivot_table.columns = [f'hour_{x[0]}_{x[1]}' for x in pivot_table.columns]\n",
    "\n",
    "count_cols = [x for x in pivot_table.columns if 'count' in x]\n",
    "pivot_table['sum'] = pivot_table[count_cols].sum(axis=1)\n",
    "for col in count_cols:\n",
    "    pivot_table[f'{col}_norm'] = pivot_table[col] / pivot_table['sum']\n",
    "pivot_table.drop('sum', axis=1, inplace=True)\n",
    "\n",
    "main = main.merge(pivot_table, how='left', left_on='user_id', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фичи, основанные на временных отрезках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = transactions.groupby('user_id')['transaction_dttm'].agg(['min', 'max']).reset_index()\n",
    "cur = cur.merge(clients[['user_id', 'report']], how='left', on='user_id')\n",
    "cur = cur.merge(report_dates, how='left', on='report')\n",
    "\n",
    "cur['min_diff_dttm'] = (cur['report_dt'] - cur['min']).dt.days\n",
    "cur['days_to_report'] = (cur['report_dt'] - cur['max']).dt.days\n",
    "cur['max_min_diff_dttm'] = cur['days_to_report'] - cur['min_diff_dttm']\n",
    "\n",
    "main = main.merge(cur[['user_id', 'min_diff_dttm','days_to_report','max_min_diff_dttm']], how='left', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main['trx_density'] = main['max_min_diff_dttm'] / main['general_trans_info_1000_count']\n",
    "main['days_density'] = (main['max_min_diff_dttm'] + 1) / main['nunique_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных к обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['employee_count_nm', 'report']\n",
    "main[cat_cols] = main[cat_cols].astype(str)\n",
    "\n",
    "main = main.sort_values('user_id').reset_index(drop=True)\n",
    "train1 = main[main.target != -1]\n",
    "test1 = main[main.target == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = '../embeddings/wtte_coles_trx.csv'\n",
    "main_embs = pd.read_csv(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train1.merge(main_embs, on='user_id')\n",
    "test1 = test1.merge(main_embs, on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = pd.read_csv('../data/test_ids.csv')\n",
    "train, test = train1.loc[~train1['user_id'].isin(test_ids['user_id'])], train1.loc[train1['user_id'].isin(test_ids['user_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отбор фичей вместе с отбором компонент эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    iterations = 1400,\n",
    "    depth=5,\n",
    "    learning_rate=0.03,\n",
    "    task_type=\"GPU\",\n",
    "    eval_metric='AUC',\n",
    "    cat_features = cat_cols,\n",
    "    thread_count=6,\n",
    "    early_stopping_rounds=200,\n",
    ")\n",
    "model.fit(train.drop(['user_id', 'target', 'time'], axis=1), train['target'], verbose=100)\n",
    "\n",
    "\n",
    "df_imp = pd.DataFrame({\n",
    "    'name': train.drop(['user_id', 'target', 'time'], axis=1).columns,\n",
    "    'imp': model.get_feature_importance()\n",
    "}).sort_values('imp', ascending=False)\n",
    "# display(df_imp) # Можно посмотреть на предварительный feature_importance()\n",
    "\n",
    "df_imp = df_imp[df_imp['imp'] > 0.15] # Берем все фичи, у которых важность больше 0.3\n",
    "\n",
    "# Добавляем статистические фичи, их нельзя было использовать для тренировки здесь, т.к. получился бы лик в данных\n",
    "good_cols = df_imp['name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение основных моделей на 5 Фолдах. Стратификация по report, возможно следует попробовать что нибудь другое:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strat_kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "X, y = train.drop(['time', 'target'], axis=1), train['target']\n",
    "scores = []\n",
    "frames_for_metamodel = []\n",
    "models = []\n",
    "for train_index, valid_index in strat_kfold.split(train, train['target']):\n",
    "    \n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    model = CatBoostClassifier(\n",
    "        iterations = 15000,\n",
    "        depth=4,\n",
    "        learning_rate=0.03,\n",
    "        eval_metric='AUC',\n",
    "        task_type=\"GPU\",\n",
    "        cat_features = cat_cols,\n",
    "        early_stopping_rounds=400,\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "    model.fit(Pool(X_train[good_cols], y_train, cat_features=cat_cols),\n",
    "              eval_set=Pool(X_val[good_cols], y_val, cat_features=cat_cols),\n",
    "              verbose=100)\n",
    "    models.append(model)\n",
    "    \n",
    "    pred = model.predict_proba(X_val[good_cols])[:, 1]\n",
    "    scores.append(metrics.roc_auc_score(y_val, pred))\n",
    "    frames_for_metamodel.append(pd.DataFrame({'user_id': X_val.user_id.values, 'pred_agg_trx': pred}))\n",
    "    scores.append(metrics.roc_auc_score(y_val, pred))\n",
    "\n",
    "print(np.mean(scores))\n",
    "metadata = pd.concat(frames_for_metamodel, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = np.zeros(len(test))\n",
    "for i in range(len(models)):\n",
    "    predict += models[i].predict_proba(test[good_cols])[:, 1]\n",
    "test_pred = {'user_id': test.user_id.values, 'pred_agg_trx': predict/5}\n",
    "test_pred = pd.DataFrame(test_pred)\n",
    "print(metrics.roc_auc_score(test['target'], predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X, y = train1.drop(['time', 'target'], axis=1), train1['target']\n",
    "scores = []\n",
    "\n",
    "models = []\n",
    "for train_index, valid_index in strat_kfold.split(train1, train1['target']):\n",
    "    \n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    model = CatBoostClassifier(\n",
    "        iterations = 15000,\n",
    "        depth=4,\n",
    "        learning_rate=0.03,\n",
    "        eval_metric='AUC',\n",
    "        task_type=\"GPU\",\n",
    "        cat_features = cat_cols,\n",
    "        early_stopping_rounds=400,\n",
    "        random_seed=42,\n",
    "    )\n",
    "\n",
    "    model.fit(Pool(X_train[good_cols], y_train, cat_features=cat_cols),\n",
    "              eval_set=Pool(X_val[good_cols], y_val, cat_features=cat_cols),\n",
    "              verbose=100)\n",
    "    models.append(model)\n",
    "    \n",
    "    pred = model.predict_proba(X_val[good_cols])[:, 1]\n",
    "    scores.append(metrics.roc_auc_score(y_val, pred))\n",
    "\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = np.zeros(len(test1))\n",
    "for i in range(len(models)):\n",
    "    predict += models[i].predict_proba(test1[good_cols])[:, 1]\n",
    "real_pred = {'user_id': test1.user_id.values, 'pred_agg_trx': predict/5}\n",
    "real_pred = pd.DataFrame(real_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.concat([metadata, test_pred, real_pred])\n",
    "metadata['pred_agg_trx'] = metadata['pred_agg_trx']\n",
    "metadata.to_csv('../predictions/agg_trx_td-idf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datafusion",
   "language": "python",
   "name": "datafusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076bdc2f-79d9-4daf-b439-7c2546e5eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from wtte_lib.wtte_data_preprocessing import data_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8477797-9742-4bf3-b7d3-14befe188020",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv('../data/transactions.csv')\n",
    "clients_df = pd.read_csv('../data/clients.csv')\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "reports_df = pd.read_csv('../data/report_dates.csv')\n",
    "transactions_df['transaction_dttm'] = pd.to_datetime(transactions_df.transaction_dttm)\n",
    "transactions_df['transaction_dttm'] = pd.to_datetime(transactions_df['transaction_dttm'], unit='s').astype('int') // 10**9\n",
    "transactions_df['mcc_code'] += 1 \n",
    "transactions_df['ones'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48602be8-c979-4bdc-8933-c0cea516196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_aggregation_dict = {'ones': 'sum', 'mcc_code': 'max', 'transaction_amt': 'mean', 'currency_rk': 'max'}\n",
    "train_data = []\n",
    "transactions_df_ = transactions_df.merge(clients_df[['user_id', 'report']])\n",
    "for report in reports_df.report.values:\n",
    "    data_ = transactions_df_[transactions_df_['report']==report].copy().reset_index()\n",
    "    df_ = data_pipeline(data_, id_col='user_id', infer_seq_endtime=False, abs_time_col='transaction_dttm', column_names=[\"ones\", 'mcc_code', 'transaction_amt', 'currency_rk'], timestep_aggregation_dict=timestep_aggregation_dict)\n",
    "    train_data.append(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85401d6-d3e7-4f1a-aa64-2170eb850356",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_data:\n",
    "    data[0][:,-1,:] = np.array([-1, -1, -1, -1])\n",
    "data_lst = []\n",
    "for data in train_data:\n",
    "    x_ = np.nan_to_num(data[0], 0).copy()\n",
    "    x_lst = [pd.DataFrame(x_[i]) for i in range(len(x_))]\n",
    "    for df in x_lst:\n",
    "        df['target'] = df[2].map(lambda x: 0 if x else None)\n",
    "        target = df.target.values\n",
    "        indices = np.where(~np.isnan(target))[0]\n",
    "        indices[-1]+=1\n",
    "        idx = 0\n",
    "        for i, tgt in enumerate(target):\n",
    "            if np.isnan(tgt):\n",
    "                target[i] = indices[idx] - i\n",
    "            else:\n",
    "                idx+=1\n",
    "        df['target'] = target\n",
    "        df.loc[df.index[-1]] = [0, 0, 0, 0, 1]\n",
    "    for i in range(len(x_lst)):\n",
    "        x_lst[i]['user_id'] = [data[2][i]]*x_lst[i].shape[0]\n",
    "    data_lst.append(pd.concat(x_lst, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70267a1f-fbf0-4072-be29-0f72f529bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat(data_lst, axis=0)\n",
    "dataset = dataset.rename(columns={0: 'ones', 1: 'mcc_code', 2: 'transaction_amt', 3: 'currency_rk'})\n",
    "dataset['trx_dt'] = dataset.groupby('user_id').cumcount()+1\n",
    "dataset['is_censored'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761cd0d-f668-47fa-b860-cdfed6c74ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.preprocessing import PandasDataPreprocessor\n",
    "\n",
    "preprocessor = PandasDataPreprocessor(\n",
    "    col_id='user_id',\n",
    "    col_event_time='trx_dt',\n",
    "    event_time_transformation='none',\n",
    "    cols_category=['mcc_code', 'currency_rk'],\n",
    "    cols_numerical=['transaction_amt', 'ones', 'target', 'is_censored'],\n",
    "    return_records=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef0f5f-19f4-413c-93aa-d81f74e23a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = preprocessor.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d5a86-d5d3-4b2d-ada6-9530b01e08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_train_part(tr_dict):\n",
    "    new_dict = []\n",
    "    for i in range(len(tr_dict)):\n",
    "        new_dict.append(dict())\n",
    "        for key, value in tr_dict[i].items():\n",
    "            if key == 'user_id':\n",
    "                new_dict[i][key] = value\n",
    "            else:\n",
    "                new_dict[i][key] = value[:150]\n",
    "    return new_dict\n",
    "train = return_train_part(tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46820365-b41b-4dc9-9ca4-0ed71536759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_valid_part(tr_dict):\n",
    "    new_dict = []\n",
    "    for i in range(len(tr_dict)):\n",
    "        new_dict.append(dict())\n",
    "        for key, value in tr_dict[i].items():\n",
    "            if key == 'user_id':\n",
    "                new_dict[i][key] = value\n",
    "            else:\n",
    "                new_dict[i][key] = value[150:]\n",
    "    return new_dict\n",
    "valid = return_valid_part(tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1350b2f-24b1-4ad3-92b1-096f0ad94ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "EPS = torch.finfo(torch.float32).eps\n",
    "\n",
    "\n",
    "def log_likelihood_discrete(tte, uncensored, alpha, beta, epsilon=EPS):\n",
    "    hazard_0 = torch.pow((tte + epsilon) / alpha, beta)\n",
    "    hazard_1 = torch.pow((tte + 1.0) / alpha, beta)\n",
    "    return uncensored * torch.log(torch.exp(hazard_1 - hazard_0) - (1.0 - epsilon)) - hazard_1\n",
    "\n",
    "\n",
    "def log_likelihood_continuous(tte, uncensored, alpha, beta, epsilon=EPS):\n",
    "    y_a = (tte + epsilon) / alpha\n",
    "    return uncensored * (torch.log(beta) + beta * torch.log(y_a)) - torch.pow(y_a, beta)\n",
    "\n",
    "\n",
    "def weibull_censored_nll_loss(\n",
    "    inputs: torch.tensor,\n",
    "    targets: torch.tensor,\n",
    "    discrete: bool = False,\n",
    "    reduction: str = \"mean\",\n",
    "    clip_prob=1e-6,\n",
    "):\n",
    "    alpha = inputs[..., 0]\n",
    "    beta = inputs[..., 1]\n",
    "    tte = targets[..., 0]\n",
    "    uncensored = targets[..., 1]\n",
    "    reducer = {\"mean\": torch.mean, \"sum\": torch.sum}.get(reduction)\n",
    "    likelihood = log_likelihood_discrete if discrete else log_likelihood_continuous\n",
    "    log_likelihoods = likelihood(tte, uncensored, alpha, beta)\n",
    "    if reducer:\n",
    "        log_likelihoods = reducer(log_likelihoods, dim=-1) / 256\n",
    "    return -1.0 * log_likelihoods\n",
    "\n",
    "\n",
    "class WeibullCensoredNLLLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, discrete: bool = False, reduction: str = \"mean\", clip_prob=1e-6):\n",
    "        super().__init__()\n",
    "        self.discrete = discrete\n",
    "        self.reduction = reduction\n",
    "        self.clip_prob = clip_prob\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: torch.tensor,\n",
    "        target: torch.tensor,\n",
    "    ):\n",
    "        return weibull_censored_nll_loss(\n",
    "            inputs, target, self.discrete, self.reduction, self.clip_prob\n",
    "        )\n",
    "\n",
    "\n",
    "class WeibullActivation(nn.Module):\n",
    "\n",
    "    def __init__(self, init_alpha: float = 1.0, max_beta: float = 5.0, epsilon: float = EPS):\n",
    "        super().__init__()\n",
    "        self.init_alpha = torch.tensor(init_alpha)\n",
    "        self.max_beta = torch.tensor(max_beta)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "\n",
    "        alpha = x[..., 0]\n",
    "        beta = x[..., 1]\n",
    "\n",
    "        alpha = self.init_alpha * torch.exp(alpha)\n",
    "\n",
    "        if self.max_beta > 1.05:\n",
    "            shift = torch.log(self.max_beta - 1.0)\n",
    "            beta = beta - shift\n",
    "\n",
    "        beta = self.max_beta * torch.clamp(\n",
    "            torch.sigmoid(beta), min=self.epsilon, max=1.0 - self.epsilon\n",
    "        )\n",
    "\n",
    "        return torch.stack([alpha, beta], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379f951-a51b-4e32-bac0-0f22a36a9bb1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828180f6-a4d7-4b7d-8e87-a6726d7f218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchmetrics\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from ptls.data_load.padded_batch import PaddedBatch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SequenceToTarget(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 seq_encoder: torch.nn.Module,\n",
    "                 head: torch.nn.Module=None,\n",
    "                 activation1=None,\n",
    "                 loss: torch.nn.Module=None,\n",
    "                 metric_list: torchmetrics.Metric=None,\n",
    "                 optimizer_partial=None,\n",
    "                 lr_scheduler_partial=None,\n",
    "                 pretrained_lr=None,\n",
    "                 train_update_n_steps=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\n",
    "            'seq_encoder', 'head', 'loss', 'metric_list', 'optimizer_partial', 'lr_scheduler_partial', 'activation1'])\n",
    "        self.activation = activation1\n",
    "        self.seq_encoder = seq_encoder\n",
    "        self.head = head\n",
    "        self.loss = loss\n",
    "\n",
    "        if type(metric_list) is dict or type(metric_list) is DictConfig:\n",
    "            metric_list = [(k, v) for k, v in metric_list.items()]\n",
    "        else:\n",
    "            if type(metric_list) is not list:\n",
    "                metric_list = [metric_list]\n",
    "            metric_list = [(m.__class__.__name__, m) for m in metric_list]\n",
    "\n",
    "        self.train_metrics = torch.nn.ModuleDict([(name, deepcopy(mc)) for name, mc in metric_list])\n",
    "        self.valid_metrics = torch.nn.ModuleDict([(name, deepcopy(mc)) for name, mc in metric_list])\n",
    "\n",
    "        self.optimizer_partial = optimizer_partial\n",
    "        self.lr_scheduler_partial = lr_scheduler_partial\n",
    "        self.val_loss = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq_encoder(x)\n",
    "        x = x.payload\n",
    "        if self.head is not None:\n",
    "            x = self.head(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_h = self(x)\n",
    "        mask = x.seq_len_mask\n",
    "        loss = self.loss(y_h[mask.bool()].unsqueeze(0), y.payload[mask.bool()].unsqueeze(0)).mean()\n",
    "        self.log('train_loss', loss, batch_size = 256)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_h = self(x)\n",
    "        mask = x.seq_len_mask\n",
    "        y_valid = y.payload[mask.bool()].unsqueeze(0)\n",
    "        y_valid[:, 1] = 1\n",
    "        loss = self.loss(y_h[mask.bool()].unsqueeze(0), y_valid).mean()\n",
    "        self.log('val_loss', loss, batch_size = 256)\n",
    "   \n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.pretrained_lr is not None:\n",
    "            if self.hparams.pretrained_lr == 'freeze':\n",
    "                for p in self.seq_encoder.parameters():\n",
    "                    p.requires_grad = False\n",
    "                logger.info('Created optimizer with frozen encoder')\n",
    "                parameters = self.parameters()\n",
    "            else:\n",
    "                parameters = [\n",
    "                    {'params': self.seq_encoder.parameters(), 'lr': self.hparams.pretrained_lr},\n",
    "                    {'params': self.head.parameters()},  # use predefined lr from `self.optimizer_partial`\n",
    "                ]\n",
    "                logger.info('Created optimizer with two lr groups')\n",
    "        else:\n",
    "            parameters = self.parameters()\n",
    "\n",
    "        optimizer = self.optimizer_partial(parameters)\n",
    "        scheduler = self.lr_scheduler_partial(optimizer)\n",
    "        return [optimizer], [scheduler]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d091ee-423b-4c6e-9b1f-164e81431797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ptls.nn import TrxEncoder, RnnSeqEncoder\n",
    "from ptls.frames.supervised import SeqToTargetDataset\n",
    "from ptls.frames.supervised.metrics import R_squared\n",
    "trx_encoder_params = dict(\n",
    "    numeric_values={'transaction_amt': 'log',\n",
    "                    'ones': 'identity',\n",
    "                   },\n",
    "    embeddings={\n",
    "        'currency_rk': {'in': 5, 'out': 2},\n",
    "        'mcc_code': {'in': 500, 'out': 16}\n",
    "    },\n",
    ")\n",
    "\n",
    "seq_encoder = RnnSeqEncoder(\n",
    "    trx_encoder=TrxEncoder(**trx_encoder_params),\n",
    "    hidden_size=16,\n",
    "    type='gru',\n",
    "    is_reduce_sequence=False,\n",
    ")\n",
    "\n",
    "model = SequenceToTarget(\n",
    "    seq_encoder=seq_encoder,\n",
    "    head = torch.nn.Sequential(torch.nn.ReLU(), torch.nn.Linear(16, 2),),\n",
    "    activation1 = WeibullActivation(init_alpha=7., max_beta=4.0),\n",
    "    loss = WeibullCensoredNLLLoss(discrete=True),\n",
    "    optimizer_partial=partial(torch.optim.AdamW, lr=0.0001),\n",
    "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=30, gamma=0.9),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7ea4d-3f70-4b43-b72b-4fb0056a9484",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0010747-6fb4-441e-b276-9ec2a80e7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ptls.data_load.padded_batch import PaddedBatch\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "from ptls.data_load.feature_dict import FeatureDict\n",
    "def collate_feature_dict(batch):\n",
    "    new_x_ = defaultdict(list)\n",
    "    for i, x in enumerate(batch):\n",
    "        for k, v in x.items():\n",
    "            new_x_[k].append(v)\n",
    "        assert reduce(\n",
    "            lambda a, b: ((a[1] is not None and a[1] == b or a[1] is None) and a[0], b),\n",
    "            map(len, new_x_.values()), (True, None))[0]\n",
    "\n",
    "    seq_col = next(k for k, v in batch[0].items() if FeatureDict.is_seq_feature(k, v))\n",
    "    lengths = torch.LongTensor([len(rec[seq_col]) for rec in batch])\n",
    "    new_x = {}\n",
    "    for k, v in new_x_.items():\n",
    "        if type(v[0]) is torch.Tensor:\n",
    "            new_x[k] = torch.nn.utils.rnn.pad_sequence(v, batch_first=True)\n",
    "        elif type(v[0]) is np.ndarray:\n",
    "            new_x[k] = v  # list of arrays[object]\n",
    "        else:\n",
    "            v = np.array(v)\n",
    "            if v.dtype.kind == 'i':\n",
    "                new_x[k] = torch.from_numpy(v).long()\n",
    "            elif v.dtype.kind == 'f':\n",
    "                new_x[k] = torch.from_numpy(v).float()\n",
    "            else:\n",
    "                new_x[k] = v\n",
    "    return PaddedBatch(new_x, lengths)\n",
    "\n",
    "class SeqToTargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 target_col_name,\n",
    "                 target_dtype=None,\n",
    "                 *args, **kwargs,\n",
    "                 ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.data = data\n",
    "        self.target_col_name = target_col_name\n",
    "        if type(target_dtype) is str:\n",
    "            self.target_dtype = getattr(torch, target_dtype)\n",
    "        else:\n",
    "            self.target_dtype = target_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        feature_arrays = self.data[item]\n",
    "        return self.sencore_last(feature_arrays)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for feature_arrays in self.data:\n",
    "            yield self.sencore_last(feature_arrays)\n",
    "    def sencore_last(self, feature_arrays):\n",
    "        if isinstance(self.target_col_name, list):\n",
    "            target = feature_arrays[self.target_col_name[0]].to(torch.int32)\n",
    "            last_tr_idx = np.max(np.where(target == target.min()))\n",
    "            new_target = torch.cat((target[:last_tr_idx+1], target[last_tr_idx+1:] - target[-1]))\n",
    "            new_is_sencored = torch.cat((torch.ones(last_tr_idx+1), torch.zeros(len(target) - last_tr_idx-1)))\n",
    "            feature_arrays[self.target_col_name[0]] = new_target\n",
    "            feature_arrays[self.target_col_name[1]] = new_is_sencored\n",
    "            return feature_arrays\n",
    "    def collate_fn(self, padded_batch):\n",
    "        padded_batch = collate_feature_dict(padded_batch)\n",
    "        if isinstance(self.target_col_name, list):\n",
    "            targets = []\n",
    "            for col in self.target_col_name:\n",
    "                targets.append(padded_batch.payload[col])\n",
    "                del padded_batch.payload[col]\n",
    "            target = torch.stack(targets, dim = 1).permute(0, 2, 1)\n",
    "        else:\n",
    "            target = padded_batch.payload[self.target_col_name]\n",
    "            del padded_batch.payload[self.target_col_name]\n",
    "        if self.target_dtype is not None:\n",
    "            target = target.to(dtype=self.target_dtype)\n",
    "        return padded_batch, PaddedBatch(target, padded_batch.seq_lens)\n",
    "\n",
    "\n",
    "class SeqToTargetIterableDataset(SeqToTargetDataset, torch.utils.data.IterableDataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692c54c-ffd6-4302-ae6b-b6d7baba4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.augmentations import RandomSlice\n",
    "class NewRandomSlice(RandomSlice):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    @staticmethod\n",
    "    def is_seq_feature(k: str, x):\n",
    "        if k == 'event_time':\n",
    "            return True\n",
    "        if type(x) in (np.ndarray, torch.Tensor):\n",
    "            return True\n",
    "        return False\n",
    "    @staticmethod\n",
    "    def seq_indexing(d, ix):\n",
    "        return {k: v[ix] if NewRandomSlice.is_seq_feature(k, v) else v for k, v in d.items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_seq_len(d):\n",
    "        if 'event_time' in d:\n",
    "            return len(d['event_time'])\n",
    "        return len(next(v for k, v in d.items() if NewRandomSlice.is_seq_feature(k, v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d757c-4e51-4fea-be87-8c17f9fdbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.data_load.datasets import AugmentationDataset\n",
    "train_dl = PtlsDataModule(\n",
    "    train_data=SeqToTargetDataset(\n",
    "        AugmentationDataset(\n",
    "            data = MemoryMapDataset(\n",
    "                data=train,\n",
    "                i_filters=[\n",
    "                ],\n",
    "            ),\n",
    "            f_augmentations = [NewRandomSlice(20, 150)]),\n",
    "        target_col_name = ['target', 'is_censored']\n",
    "    ),\n",
    "    valid_data=SeqToTargetDataset(\n",
    "        MemoryMapDataset(\n",
    "            data=valid,\n",
    "            i_filters=[\n",
    "            ],\n",
    "        ),\n",
    "        target_col_name = ['target', 'is_censored']\n",
    "    ),\n",
    "    train_num_workers=0,\n",
    "    train_batch_size=12000,\n",
    "    valid_num_workers=0,\n",
    "    valid_batch_size=12000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b4be2-34d0-4d73-9473-ddd3897d71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=40,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    enable_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1831026-0322-40e1-b392-e7d3644c8537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f'logger.version = {trainer.logger.version}')\n",
    "trainer.fit(model, train_dl)\n",
    "print(trainer.logged_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb391b13-1ee2-40b0-9a91-c36aa0db11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tqdm\n",
    "from ptls.data_load.datasets import inference_data_loader\n",
    "import numpy as np\n",
    "\n",
    "def pooling_inference(model, dl, device='cuda:0'):\n",
    "    \n",
    "    model.to(device)\n",
    "    X = []\n",
    "    for batch in tqdm.tqdm(dl):\n",
    "        with torch.no_grad():\n",
    "            x = model.seq_encoder.trx_encoder(batch.to(device)).payload\n",
    "            out_max = torch.max(x, dim=1)[0]\n",
    "            out_min = torch.min(x, dim=1)[0]\n",
    "            out_mean = torch.mean(x, dim=1)\n",
    "            out_std = torch.std(x, dim=1)\n",
    "            features = torch.cat([out_max, out_min, out_mean, out_std], dim=1)      \n",
    "            X += [features]\n",
    "    return X\n",
    "\n",
    "def embed_inference(model, dl, device='cuda:0'):\n",
    "    \n",
    "    model.to(device)\n",
    "    X = []\n",
    "    for batch in tqdm.tqdm(dl):\n",
    "        with torch.no_grad():\n",
    "            features = model.seq_encoder(batch.to(device)).payload\n",
    "            x = model.head(features)\n",
    "            features = []\n",
    "            for i in range(0, 150, 30):\n",
    "                x_30 = x[:, i:i+30]\n",
    "                out_max = torch.max(x_30, dim=1)[0]\n",
    "                out_min = torch.min(x_30, dim=1)[0]\n",
    "                out_mean = torch.mean(x_30, dim=1)\n",
    "                out_std = torch.std(x_30, dim=1)\n",
    "                features.append(torch.cat([out_max, out_min, out_mean, out_std], dim=1)) \n",
    "            for i in range(150, 182, 5):\n",
    "                x_30 = x[:, i:]\n",
    "                out_max = torch.max(x_30, dim=1)[0]\n",
    "                out_min = torch.min(x_30, dim=1)[0]\n",
    "                out_mean = torch.mean(x_30, dim=1)\n",
    "                out_std = torch.std(x_30, dim=1)\n",
    "                features.append(torch.cat([out_max, out_min, out_mean, out_std], dim=1)) \n",
    "                \n",
    "            features = torch.cat(features, dim = 1)\n",
    "            features = torch.cat([features, x[:, -1]], dim = 1)\n",
    "            X += [features]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348c809-d34e-4124-a60a-8b29d37f7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.seq_encoder.is_reduce_sequence = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb84a1-c821-46cd-bc05-8a15237dcff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl = inference_data_loader(tr_dataset, num_workers=0, batch_size=512)\n",
    "df_ab = torch.vstack(embed_inference(model, dl)).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d2773-45ac-467b-aa4f-e2a58d39d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeds = pd.DataFrame(df_ab, columns=[f\"ab_emb_{e}\" for e in range(df_ab.shape[1])])\n",
    "df_embeds['user_id'] = pd.DataFrame(tr_dataset)['user_id']\n",
    "df_embeds.to_csv('../embeddings/wtte_rnn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESF",
   "language": "python",
   "name": "esf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from omegaconf import DictConfig\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ptls.nn import TrxEncoder, RnnSeqEncoder, Head, L2NormEncoder\n",
    "from ptls.data_load.utils import collate_feature_dict\n",
    "from ptls.data_load.datasets import MemoryMapDataset, AugmentationDataset\n",
    "from ptls.data_load.padded_batch import PaddedBatch\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.preprocessing import PandasDataPreprocessor\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.frames.coles import CoLESModule, ColesDataset\n",
    "from ptls.frames.coles.losses import SoftmaxLoss\n",
    "from ptls.frames.coles.metric import BatchRecallTopK\n",
    "from ptls.frames.coles.split_strategy import SampleSlices\n",
    "from ptls.frames.inference_module import InferenceModule\n",
    "from ptls.data_load.augmentations import RandomSlice, DropoutTrx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv('../data/transactions.csv')\n",
    "clients_df = pd.read_csv('../data/clients.csv')\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "reports_df = pd.read_csv('../data/report_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df['transaction_dttm'] = pd.to_datetime(transactions_df.transaction_dttm)\n",
    "report_dates = pd.read_csv('../data/report_dates.csv', parse_dates=['report_dt'])\n",
    "df_ = transactions_df.merge(clients_df[['user_id', 'report']], how='left', on='user_id')\n",
    "df_ = df_.merge(report_dates, how='left', on='report')\n",
    "transactions_df['days_to_report'] = (df_['report_dt'] - df_['transaction_dttm']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодируем день недели, добавляем флаг выходного дня\n",
    "days_of_week = {'Monday': 1,\n",
    "                'Tuesday': 2,\n",
    "                'Wednesday': 3,\n",
    "                'Thursday': 4,\n",
    "                'Friday': 5,\n",
    "                'Saturday': 6,\n",
    "                'Sunday': 7\n",
    "               }\n",
    "\n",
    "transactions_df['day_of_week'] = transactions_df['transaction_dttm'].dt.day_name()\n",
    "for k, v in days_of_week.items():\n",
    "    transactions_df['day_of_week'].replace(k,v,inplace= True)\n",
    "    \n",
    "transactions_df[\"is_day_off\"] = transactions_df['day_of_week'].map(lambda x: 1 if x in (6,7) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем количество дней, часов с момента первой и предыдущей транзакций\n",
    "first_trx = transactions_df.groupby('user_id')['transaction_dttm'].min().reset_index()\n",
    "first_trx.rename(columns={'transaction_dttm': 'first_tr'}, inplace=True)\n",
    "transactions_df = transactions_df.merge(first_trx, on='user_id', how='left')\n",
    "\n",
    "transactions_df['days_from_first_tr'] = (transactions_df['transaction_dttm']-transactions_df['first_tr'])/ np.timedelta64(1, 'D')\n",
    "transactions_df['days_from_first_tr'] = (transactions_df['days_from_first_tr']).astype('int')\n",
    "transactions_df['days_from_prev_tr'] = transactions_df['transaction_dttm'].diff()/ np.timedelta64(1, 'D')\n",
    "transactions_df['days_from_prev_tr'] = transactions_df['days_from_prev_tr'].fillna(0)\n",
    "\n",
    "transactions_df['days_from_prev_tr'] = (transactions_df['days_from_prev_tr']).astype('int')\n",
    "\n",
    "transactions_df['hours_from_first_tr'] = (transactions_df['transaction_dttm']-transactions_df['first_tr'])/ np.timedelta64(1, 'h')\n",
    "transactions_df['hours_from_prev_tr'] = transactions_df['transaction_dttm'].diff()/ np.timedelta64(1, 'h')\n",
    "transactions_df['hours_from_prev_tr'] = transactions_df['hours_from_prev_tr'].fillna(0)\n",
    "transactions_df['hour'] = transactions_df['transaction_dttm'].dt.hour\n",
    "\n",
    "transactions_df = transactions_df.drop(columns=['first_tr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_ = ['mcc_code',\n",
    "             'currency_rk',\n",
    "             'day_of_week',\n",
    "             'is_day_off',\n",
    "             'hour'\n",
    "            ]\n",
    "num_cols_ = ['transaction_amt',\n",
    "              'days_from_first_tr',\n",
    "              'days_from_prev_tr',\n",
    "              'hours_from_first_tr',\n",
    "              'hours_from_prev_tr',\n",
    "            ]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_preprocessor = PandasDataPreprocessor(\n",
    "    col_id='user_id',\n",
    "    col_event_time='transaction_dttm',\n",
    "    event_time_transformation='dt_to_timestamp',\n",
    "    cols_category=[\n",
    "        'mcc_code',\n",
    "        'currency_rk',\n",
    "        'day_of_week',\n",
    "        'is_day_off',\n",
    "        'hour',\n",
    "    ],\n",
    "    cols_numerical=[\n",
    "        'transaction_amt',\n",
    "        'days_from_first_tr',\n",
    "        'days_from_prev_tr',\n",
    "        'hours_from_first_tr',\n",
    "        'hours_from_prev_tr',\n",
    "        'days_to_report',\n",
    "    ],\n",
    "    return_records=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tr_dataset = trx_preprocessor.fit_transform(transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tr_dataset.merge(train_df[['user_id', 'target', 'time']], on='user_id')\n",
    "test_ids = pd.read_csv('../data/test_ids.csv')\n",
    "df_train, df_test = df.loc[~df['user_id'].isin(test_ids['user_id'])], df.loc[df['user_id'].isin(test_ids['user_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coles Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coles_train_df, coles_valid_df = train_test_split(tr_dataset, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coles_dataset = PtlsDataModule(\n",
    "    train_data=ColesDataset(\n",
    "        AugmentationDataset(\n",
    "            coles_train_df.to_dict(orient='records'),\n",
    "            f_augmentations=[\n",
    "                DropoutTrx(0.1),\n",
    "            ],\n",
    "        ),\n",
    "        splitter=SampleSlices(5, 10, 80),\n",
    "    ),\n",
    "    valid_data=ColesDataset(\n",
    "        coles_valid_df.to_dict(orient='records'),\n",
    "        splitter=SampleSlices(5, 10, 80),\n",
    "    ),\n",
    "    train_batch_size=256,\n",
    "    train_num_workers=8,\n",
    "    train_drop_last=True,\n",
    "    \n",
    "    valid_batch_size=256,\n",
    "    valid_num_workers=8,\n",
    "    valid_drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_encoder_params = dict(\n",
    "    numeric_values={\n",
    "        'transaction_amt': 'log',\n",
    "        'days_from_first_tr': 'identity',\n",
    "        'days_from_prev_tr': 'log',\n",
    "        'hours_from_first_tr': 'identity',\n",
    "        'hours_from_prev_tr': 'log',\n",
    "        'days_to_report': 'log',\n",
    "    },\n",
    "    embeddings={\n",
    "        'currency_rk': {'in': 5, 'out': 16},\n",
    "        'day_of_week': {'in': 8, 'out': 16},\n",
    "        'mcc_code': {'in': 330, 'out': 32},\n",
    "        'is_day_off': {'in': 4, 'out': 2},\n",
    "        'hour': {'in': 30, 'out': 4}\n",
    "    },\n",
    "    use_batch_norm_with_lens=True,\n",
    ")\n",
    "\n",
    "seq_encoder = RnnSeqEncoder(\n",
    "    trx_encoder=TrxEncoder(**trx_encoder_params),\n",
    "    hidden_size=800,\n",
    "    type='gru',\n",
    ")\n",
    "\n",
    "# seq_encoder.load_state_dict(torch.load(os.path.join(PATH, 'model/seq_encoder.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coles_module = CoLESModule(\n",
    "    seq_encoder=seq_encoder,\n",
    "    head=Head(use_norm_encoder=True),\n",
    "    loss=SoftmaxLoss(),\n",
    "    validation_metric=BatchRecallTopK(K=4),\n",
    "    optimizer_partial=partial(torch.optim.AdamW, lr=1e-3, weight_decay=1e-4),\n",
    "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=10, gamma=0.9),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    gpus=1,\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_algorithm='value',\n",
    "    gradient_clip_val=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(coles_module, coles_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(seq_encoder.state_dict(), '../models/seq_encoder-0.1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqToTargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 target_col_name,\n",
    "                 target_dtype=None,\n",
    "                 *args, **kwargs,\n",
    "                 ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "        self.target_col_name = target_col_name\n",
    "        if type(target_dtype) is str:\n",
    "            self.target_dtype = getattr(torch, target_dtype)\n",
    "        else:\n",
    "            self.target_dtype = target_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        feature_arrays = self.data[item]\n",
    "        return feature_arrays\n",
    "\n",
    "    def __iter__(self):\n",
    "        for feature_arrays in self.data:\n",
    "            yield feature_arrays\n",
    "\n",
    "    def collate_fn(self, padded_batch):\n",
    "        padded_batch = collate_feature_dict(padded_batch)\n",
    "        \n",
    "        target = padded_batch.payload[self.target_col_name]\n",
    "        time = padded_batch.payload['time']\n",
    "        del padded_batch.payload[self.target_col_name]\n",
    "        if self.target_dtype is not None:\n",
    "            target = target.to(dtype=self.target_dtype)\n",
    "\n",
    "        return padded_batch, target, time\n",
    "\n",
    "\n",
    "class SeqToTargetIterableDataset(SeqToTargetDataset, torch.utils.data.IterableDataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceToTarget(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_encoder: torch.nn.Module,\n",
    "        head: torch.nn.Module=None,\n",
    "        head_time: torch.nn.Module=None,\n",
    "        loss: torch.nn.Module=None,\n",
    "        metric_list: torchmetrics.Metric=None,\n",
    "        optimizer_partial=None,\n",
    "        lr_scheduler_partial=None,\n",
    "        pretrained_lr=None,\n",
    "        train_update_n_steps=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\n",
    "            'seq_encoder', 'head', 'head_time', 'loss',\n",
    "            'metric_list', 'optimizer_partial', 'lr_scheduler_partial'\n",
    "        ])\n",
    "        self.seq_encoder = seq_encoder\n",
    "        self.head = head\n",
    "        self.head_time = head_time\n",
    "        self.loss = loss\n",
    "\n",
    "        if type(metric_list) is dict or type(metric_list) is DictConfig:\n",
    "            metric_list = [(k, v) for k, v in metric_list.items()]\n",
    "        else:\n",
    "            if type(metric_list) is not list:\n",
    "                metric_list = [metric_list]\n",
    "            metric_list = [(m.__class__.__name__, m) for m in metric_list]\n",
    "\n",
    "        self.train_metrics = torch.nn.ModuleDict([(name, deepcopy(mc)) for name, mc in metric_list])\n",
    "        self.valid_metrics = torch.nn.ModuleDict([(name, deepcopy(mc)) for name, mc in metric_list])\n",
    "\n",
    "        self.optimizer_partial = optimizer_partial\n",
    "        self.lr_scheduler_partial = lr_scheduler_partial\n",
    "        \n",
    "    def forward(self, x):\n",
    "        add_features = None\n",
    "        \n",
    "        if isinstance(x, tuple):\n",
    "            x, add_features = x\n",
    "\n",
    "        x = self.seq_encoder(x)\n",
    "        \n",
    "        if self.head is not None:\n",
    "            y_h = self.head(x)\n",
    "        else:\n",
    "            y_h = x\n",
    "            \n",
    "        t_h = self.head_time(x)\n",
    "        \n",
    "        return y_h, t_h\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y, t = batch\n",
    "        y_h, t_h = self(x)\n",
    "\n",
    "        loss = self.loss(y_h, y)\n",
    "        self.log('loss/train_loss', loss)\n",
    "        for name, mf in self.train_metrics.items():\n",
    "            mf(y_h, y)\n",
    "            \n",
    "        loss_t = (t_h - t / 100.0).pow(2).mean()\n",
    "        self.log('loss/loss_time', loss_t)\n",
    "        return loss + 0.1 * loss_t\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        for name, mf in self.train_metrics.items():\n",
    "            self.log(f'{name}/train', mf.compute(), prog_bar=False)\n",
    "        for name, mf in self.train_metrics.items():\n",
    "            mf.reset()\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y, t = batch\n",
    "        y_h, t_h = self(x)\n",
    "        self.log('loss/valid', self.loss(y_h, y))\n",
    "        for name, mf in self.valid_metrics.items():\n",
    "            mf(y_h, y)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        for name, mf in self.valid_metrics.items():\n",
    "            self.log(f'{name}/valid', mf.compute(), prog_bar=True)\n",
    "        for name, mf in self.valid_metrics.items():\n",
    "            mf.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.pretrained_lr is not None:\n",
    "            if self.hparams.pretrained_lr == 'freeze':\n",
    "                for p in self.seq_encoder.parameters():\n",
    "                    p.requires_grad = False\n",
    "                parameters = self.parameters()\n",
    "            else:\n",
    "                parameters = [\n",
    "                    {'params': self.seq_encoder.parameters(), 'lr': self.hparams.pretrained_lr},\n",
    "                    {'params': self.head.parameters()},  # use predefined lr from `self.optimizer_partial`\n",
    "                ]\n",
    "        else:\n",
    "            parameters = self.parameters()\n",
    "\n",
    "        optimizer = self.optimizer_partial(parameters)\n",
    "        scheduler = self.lr_scheduler_partial(optimizer)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, dl, device='cuda:0'):\n",
    "    logits = []\n",
    "    model.to(device)\n",
    "    softmax = torch.nn.Softmax(dim=0) \n",
    "    for batch in tqdm.tqdm(dl, position=0, leave=True):\n",
    "        with torch.no_grad():\n",
    "            x, _, _ = batch\n",
    "            y_h, t_h = model(x.to(device))\n",
    "            logits.extend([y_h.cpu()])\n",
    "        \n",
    "    logits = softmax(torch.vstack(logits)[:, 1]).cpu()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "models = []\n",
    "predictions_5folds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, (train_index, test_index) in enumerate(skf.split(df_train.drop(columns=['target']), df_train['target'])):\n",
    "    train_, test_ = df_train.iloc[train_index], df_train.iloc[test_index]\n",
    "    \n",
    "    dataset_train = train_.to_dict(orient='records')\n",
    "    dataset_test = test_.to_dict(orient='records')\n",
    "    \n",
    "    sup_dataset = PtlsDataModule(\n",
    "        train_data=SeqToTargetDataset(\n",
    "            AugmentationDataset(\n",
    "                dataset_train,\n",
    "                f_augmentations=[\n",
    "                    DropoutTrx(0.1),\n",
    "                ],\n",
    "            ),\n",
    "            target_col_name='target',\n",
    "            target_dtype=torch.long,\n",
    "        ),\n",
    "        valid_data=SeqToTargetDataset(\n",
    "            dataset_test,\n",
    "            target_col_name='target',\n",
    "            target_dtype=torch.long,\n",
    "        ),\n",
    "        train_batch_size=256,\n",
    "        train_num_workers=8,\n",
    "        train_drop_last=True,\n",
    "\n",
    "        valid_batch_size=256,\n",
    "        valid_num_workers=8,\n",
    "        valid_drop_last=True\n",
    "    )\n",
    "    \n",
    "    seq_encoder.load_state_dict(torch.load('../models/seq_encoder-0.1.pt'))\n",
    "\n",
    "    sup_module = SequenceToTarget(\n",
    "        seq_encoder=seq_encoder,\n",
    "        head=torch.nn.Sequential(\n",
    "            torch.nn.Linear(seq_encoder.embedding_size, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(512, 2),\n",
    "            torch.nn.LogSoftmax(dim=1),\n",
    "        ),\n",
    "        head_time=torch.nn.Sequential(\n",
    "            torch.nn.Linear(seq_encoder.embedding_size, 1),\n",
    "        ),\n",
    "        loss=torch.nn.NLLLoss(),\n",
    "        metric_list=torchmetrics.AUROC(num_classes=2),\n",
    "        optimizer_partial=partial(torch.optim.AdamW, lr=1e-3, weight_decay=0.0),\n",
    "        lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=1, gamma=0.2),\n",
    "    )\n",
    "    \n",
    "    # sup_module.load_state_dict(torch.load(f\"model/sup_modules-kfold/model-1.{i}.pt\"))\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=TensorBoardLogger('lightning_logs', name=f'CoLES-supervised-agg_{i}'),\n",
    "        max_epochs=6,\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        enable_progress_bar=True,\n",
    "        gradient_clip_algorithm='norm',\n",
    "        gradient_clip_val=0.18\n",
    "    )\n",
    "    \n",
    "    trainer.fit(sup_module, sup_dataset)\n",
    "    \n",
    "    torch.save(sup_module.state_dict(), f\"../models/sup_modules-kfold.{i}.pt\")\n",
    "    \n",
    "    predictions_test = test_[[\"user_id\"]].copy()\n",
    "    \n",
    "    dataset = SeqToTargetDataset(\n",
    "        data=dataset_test,\n",
    "        target_col_name='target',\n",
    "    )\n",
    "\n",
    "    dl = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        shuffle=False,\n",
    "        batch_size=512,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    \n",
    "    predictions_test[\"sp\"] = inference(sup_module, dl)\n",
    "    \n",
    "    predictions_5folds.append(predictions_test)\n",
    "    \n",
    "    print(12*\"-\")\n",
    "    print(\"AUROC; 5th fold:\", roc_auc_score(test_[\"target\"].values, predictions_test[\"sp\"]))\n",
    "    print(12*\"-\")\n",
    "    \n",
    "    models.append(deepcopy(sup_module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_5folds = pd.concat(predictions_5folds, axis=0)\n",
    "temp = predictions_5folds.merge(train_df[[\"user_id\", \"target\"]], on=\"user_id\")\n",
    "print(12*\"-\")\n",
    "print(\"AUROC; 5th fold:\", roc_auc_score(temp[\"target\"].values, temp[\"sp\"].values))\n",
    "print(12*\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = df_test.copy()\n",
    "dataset_test[[\"target\", \"time\"]] = None\n",
    "dataset_test = dataset_test.to_dict(orient='records')\n",
    "\n",
    "dataset = SeqToTargetDataset(\n",
    "    data=dataset_test,\n",
    "    target_col_name='target',\n",
    ")\n",
    "\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    shuffle=False,\n",
    "    batch_size=512,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "predictions_test = df_test[[\"user_id\"]].copy()\n",
    "\n",
    "for i in range(5):\n",
    "    predictions_test[f\"sp_{i}\"] = inference(models[i], dl)\n",
    "predictions_test[\"sp\"] = predictions_test.iloc[:, 1:].mean(axis=1)\n",
    "predictions_test\n",
    "print(12*\"-\")\n",
    "print(\"AUROC; 5th fold:\", roc_auc_score(df_test[\"target\"].values, predictions_test[\"sp\"].values))\n",
    "print(12*\"-\")\n",
    "train_test_predictions = pd.concat([predictions_5folds, predictions_test[[\"user_id\", \"sp\"]]], axis=0)\n",
    "#train_test_predictions.to_csv(\"sp-preds_train-test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submit = pd.read_csv(\"../data/sample_submit_naive.csv\")\n",
    "sbmt_df = tr_dataset.merge(sample_submit[['user_id']], on='user_id')\n",
    "sbmt_df[[\"target\", \"time\"]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbmt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbmt_models = []\n",
    "for i, (train_index, test_index) in enumerate(skf.split(df.drop(columns=['target']), df[\"target\"])):\n",
    "    train_, test_ = df.iloc[train_index], df.iloc[test_index]\n",
    "    \n",
    "    dataset_train = train_.to_dict(orient='records')\n",
    "    dataset_test = test_.to_dict(orient='records')\n",
    "    \n",
    "    sup_dataset = PtlsDataModule(\n",
    "        train_data=SeqToTargetDataset(\n",
    "            AugmentationDataset(\n",
    "                dataset_train,\n",
    "                f_augmentations=[\n",
    "                    DropoutTrx(0.1),\n",
    "                ],\n",
    "            ),\n",
    "            target_col_name='target',\n",
    "            target_dtype=torch.long,\n",
    "        ),\n",
    "        valid_data=SeqToTargetDataset(\n",
    "            dataset_test,\n",
    "            target_col_name='target',\n",
    "            target_dtype=torch.long,\n",
    "        ),\n",
    "        train_batch_size=256,\n",
    "        train_num_workers=8,\n",
    "        train_drop_last=True,\n",
    "\n",
    "        valid_batch_size=256,\n",
    "        valid_num_workers=8,\n",
    "        valid_drop_last=True\n",
    "    )\n",
    "    \n",
    "    seq_encoder.load_state_dict(torch.load('../models/seq_encoder-0.1.pt'))\n",
    "\n",
    "    sup_module = SequenceToTarget(\n",
    "        seq_encoder=seq_encoder,\n",
    "        head=torch.nn.Sequential(\n",
    "            torch.nn.Linear(seq_encoder.embedding_size, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(512, 2),\n",
    "            torch.nn.LogSoftmax(dim=1),\n",
    "        ),\n",
    "        head_time=torch.nn.Sequential(\n",
    "            torch.nn.Linear(seq_encoder.embedding_size, 1),\n",
    "        ),\n",
    "        loss=torch.nn.NLLLoss(),\n",
    "        metric_list=torchmetrics.AUROC(num_classes=2),\n",
    "        optimizer_partial=partial(torch.optim.AdamW, lr=4e-4, weight_decay=0.0),\n",
    "        lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=1, gamma=0.2),\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=TensorBoardLogger('lightning_logs', name=f'CoLES-supervised-agg-sbmt_{i}'),\n",
    "        max_epochs=12,\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        enable_progress_bar=True,\n",
    "        gradient_clip_algorithm='norm',\n",
    "        gradient_clip_val=0.2\n",
    "    )\n",
    "    \n",
    "    trainer.fit(sup_module, sup_dataset)\n",
    "    \n",
    "    #torch.save(sup_module.state_dict(), f\"model/sup_modules-kfold/sbmt-model-0.{i}.pt\")\n",
    "    \n",
    "    dataset = SeqToTargetDataset(\n",
    "        data=dataset_test,\n",
    "        target_col_name='target',\n",
    "    )\n",
    "\n",
    "    dl = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        shuffle=False,\n",
    "        batch_size=512,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    \n",
    "    predictions_test = inference(sup_module, dl)\n",
    "    \n",
    "    print(12*\"-\")\n",
    "    print(\"AUROC; 5th fold:\", roc_auc_score(test_[\"target\"].values, predictions_test))\n",
    "    print(12*\"-\")\n",
    "    \n",
    "    sbmt_models.append(deepcopy(sup_module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sbmt = sbmt_df.copy()\n",
    "dataset_sbmt[[\"target\", \"time\"]] = None\n",
    "dataset_sbmt = dataset_sbmt.to_dict(orient='records')\n",
    "\n",
    "dataset = SeqToTargetDataset(\n",
    "    data=dataset_sbmt,\n",
    "    target_col_name='target',\n",
    ")\n",
    "\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    shuffle=False,\n",
    "    batch_size=512,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "predictions_sbmt = sbmt_df[[\"user_id\"]].copy()\n",
    "\n",
    "for i in range(5):\n",
    "    predictions_sbmt[f\"sp_{i}\"] = inference(sbmt_models[i], dl)\n",
    "\n",
    "predictions_sbmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sbmt[\"sp\"] = predictions_sbmt.iloc[:, 1:].mean(axis=1)\n",
    "predictions_sbmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = pd.concat([predictions_sbmt[[\"user_id\", \"sp\"]], train_test_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction.to_csv(\"../predictions/sup-preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kalash_cuda116_ptls",
   "language": "python",
   "name": "kalash_cuda116_ptls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

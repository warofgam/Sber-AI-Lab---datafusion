{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n",
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from wtte_lib.wtte_data_preprocessing import data_pipeline\n",
    "from ptls.nn import TrxEncoder, RnnSeqEncoder, Head, L2NormEncoder\n",
    "from ptls.data_load.utils import collate_feature_dict\n",
    "from ptls.data_load.datasets import MemoryMapDataset, AugmentationDataset\n",
    "from ptls.data_load.padded_batch import PaddedBatch\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.preprocessing import PandasDataPreprocessor\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.frames.coles import CoLESModule, ColesDataset\n",
    "from ptls.frames.coles.losses import SoftmaxLoss\n",
    "from ptls.frames.coles.metric import BatchRecallTopK\n",
    "from ptls.frames.coles.split_strategy import SampleSlices\n",
    "from ptls.frames.inference_module import InferenceModule\n",
    "from ptls.data_load.augmentations import RandomSlice, DropoutTrx\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = pd.read_csv('../data/transactions.csv')\n",
    "clients_df = pd.read_csv('../data/clients.csv')\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "reports_df = pd.read_csv('../data/report_dates.csv')\n",
    "transactions_df['transaction_dttm'] = pd.to_datetime(transactions_df.transaction_dttm)\n",
    "transactions_df['transaction_dttm'] = pd.to_datetime(transactions_df['transaction_dttm'], unit='s').astype('int') // 10**9\n",
    "transactions_df['mcc_code'] += 1 \n",
    "transactions_df['ones'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_aggregation_dict = {'ones': 'sum', 'mcc_code': 'max', 'transaction_amt': 'mean', 'currency_rk': 'max'}\n",
    "train_data = []\n",
    "transactions_df_ = transactions_df.merge(clients_df[['user_id', 'report']])\n",
    "for report in reports_df.report.values:\n",
    "    data_ = transactions_df_[transactions_df_['report']==report].copy().reset_index()\n",
    "    df_ = data_pipeline(data_, id_col='user_id', infer_seq_endtime=False, abs_time_col='transaction_dttm', column_names=[\"ones\", 'mcc_code', 'transaction_amt', 'currency_rk'], timestep_aggregation_dict=timestep_aggregation_dict)\n",
    "    train_data.append(df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_data:\n",
    "    data[0][:,-1,:] = np.array([-1, -1, -1, -1])\n",
    "data_lst = []\n",
    "for data in train_data:\n",
    "    x_ = np.nan_to_num(data[0], 0).copy()\n",
    "    x_lst = [pd.DataFrame(x_[i]) for i in range(len(x_))]\n",
    "    for df in x_lst:\n",
    "        df['target'] = df[2].map(lambda x: 0 if x else None)\n",
    "        target = df.target.values\n",
    "        indices = np.where(~np.isnan(target))[0]\n",
    "        indices[-1]+=1\n",
    "        idx = 0\n",
    "        for i, tgt in enumerate(target):\n",
    "            if np.isnan(tgt):\n",
    "                target[i] = indices[idx] - i\n",
    "            else:\n",
    "                idx+=1\n",
    "        df['target'] = target\n",
    "        df.loc[df.index[-1]] = [0, 0, 0, 0, 1]\n",
    "    for i in range(len(x_lst)):\n",
    "        x_lst[i]['user_id'] = [data[2][i]]*x_lst[i].shape[0]\n",
    "    data_lst.append(pd.concat(x_lst, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat(data_lst, axis=0)\n",
    "dataset = dataset.rename(columns={0: 'ones', 1: 'mcc_code', 2: 'transaction_amt', 3: 'currency_rk'})\n",
    "dataset['trx_dt'] = dataset.groupby('user_id').cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.preprocessing import PandasDataPreprocessor\n",
    "\n",
    "preprocessor = PandasDataPreprocessor(\n",
    "    col_id='user_id',\n",
    "    col_event_time='trx_dt',\n",
    "    event_time_transformation='none',\n",
    "    cols_category=['mcc_code', 'currency_rk'],\n",
    "    cols_numerical=['transaction_amt', 'ones'],\n",
    "    return_records=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = preprocessor.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tr_dataset).drop('target', axis=1).merge(train_df[['user_id', 'target', 'time']], on='user_id')\n",
    "test_ids = pd.read_csv('../data/test_ids.csv')\n",
    "df_train, df_test = df.loc[~df['user_id'].isin(test_ids['user_id'])], df.loc[df['user_id'].isin(test_ids['user_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_encoder_params = dict(\n",
    "    embeddings_noise=0.003,\n",
    "    numeric_values={'transaction_amt': 'identity',\n",
    "                    'ones': 'identity',\n",
    "                   },\n",
    "    embeddings={\n",
    "        'currency_rk': {'in': 5, 'out': 4},\n",
    "        'mcc_code': {'in': 333, 'out': 8},\n",
    "    },\n",
    ")\n",
    "\n",
    "seq_encoder = RnnSeqEncoder(\n",
    "    trx_encoder=TrxEncoder(**trx_encoder_params),\n",
    "    hidden_size=800,\n",
    "    type='gru',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceToTarget(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_encoder: torch.nn.Module,\n",
    "        head: torch.nn.Module=None,\n",
    "        head_time: torch.nn.Module=None,\n",
    "        loss: torch.nn.Module=None,\n",
    "        metric_list: torchmetrics.Metric=None,\n",
    "        optimizer_partial=None,\n",
    "        lr_scheduler_partial=None,\n",
    "        pretrained_lr=None,\n",
    "        train_update_n_steps=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\n",
    "            'seq_encoder', 'head', 'head_time', 'loss',\n",
    "            'metric_list', 'optimizer_partial', 'lr_scheduler_partial'\n",
    "        ])\n",
    "        self.seq_encoder = seq_encoder\n",
    "        self.head = head\n",
    "        self.head_time = head_time\n",
    "        self.loss = loss\n",
    "\n",
    "        if type(metric_list) is dict or type(metric_list) is DictConfig:\n",
    "            metric_list = [(k, v) for k, v in metric_list.items()]\n",
    "        else:\n",
    "            if type(metric_list) is not list:\n",
    "                metric_list = [metric_list]\n",
    "            metric_list = [(m.__class__.__name__, m) for m in metric_list]\n",
    "\n",
    "        self.train_metrics = torch.nn.ModuleDict([(name, deepcopy(mc)) for name, mc in metric_list])\n",
    "        self.valid_metrics = torch.nn.ModuleDict([(name, deepcopy(mc)) for name, mc in metric_list])\n",
    "\n",
    "        self.optimizer_partial = optimizer_partial\n",
    "        self.lr_scheduler_partial = lr_scheduler_partial\n",
    "        \n",
    "    def forward(self, x):\n",
    "        add_features = None\n",
    "        \n",
    "        if isinstance(x, tuple):\n",
    "            x, add_features = x\n",
    "\n",
    "        x = self.seq_encoder(x)\n",
    "        \n",
    "        if self.head is not None:\n",
    "            y_h = self.head(x)\n",
    "        else:\n",
    "            y_h = x\n",
    "            \n",
    "        t_h = self.head_time(x)\n",
    "        \n",
    "        return y_h, t_h\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y, t = batch\n",
    "        y_h, t_h = self(x)\n",
    "\n",
    "        loss = self.loss(y_h, y)\n",
    "        self.log('loss/train_loss', loss)\n",
    "        for name, mf in self.train_metrics.items():\n",
    "            mf(y_h, y)\n",
    "            \n",
    "        loss_t = (t_h - t / 100.0).pow(2).mean()\n",
    "        self.log('loss/loss_time', loss_t)\n",
    "        return loss + 0.1 * loss_t\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        for name, mf in self.train_metrics.items():\n",
    "            self.log(f'{name}/train', mf.compute(), prog_bar=False)\n",
    "        for name, mf in self.train_metrics.items():\n",
    "            mf.reset()\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y, t = batch\n",
    "        y_h, t_h = self(x)\n",
    "        self.log('loss/valid', self.loss(y_h, y))\n",
    "        for name, mf in self.valid_metrics.items():\n",
    "            mf(y_h, y)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        for name, mf in self.valid_metrics.items():\n",
    "            self.log(f'{name}/valid', mf.compute(), prog_bar=True)\n",
    "        for name, mf in self.valid_metrics.items():\n",
    "            mf.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.pretrained_lr is not None:\n",
    "            if self.hparams.pretrained_lr == 'freeze':\n",
    "                for p in self.seq_encoder.parameters():\n",
    "                    p.requires_grad = False\n",
    "                parameters = self.parameters()\n",
    "            else:\n",
    "                parameters = [\n",
    "                    {'params': self.seq_encoder.parameters(), 'lr': self.hparams.pretrained_lr},\n",
    "                    {'params': self.head.parameters()},  # use predefined lr from `self.optimizer_partial`\n",
    "                ]\n",
    "        else:\n",
    "            parameters = self.parameters()\n",
    "\n",
    "        optimizer = self.optimizer_partial(parameters)\n",
    "        scheduler = self.lr_scheduler_partial(optimizer)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqToTargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 target_col_name,\n",
    "                 target_dtype=None,\n",
    "                 *args, **kwargs,\n",
    "                 ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "        self.target_col_name = target_col_name\n",
    "        if type(target_dtype) is str:\n",
    "            self.target_dtype = getattr(torch, target_dtype)\n",
    "        else:\n",
    "            self.target_dtype = target_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        feature_arrays = self.data[item]\n",
    "        return feature_arrays\n",
    "\n",
    "    def __iter__(self):\n",
    "        for feature_arrays in self.data:\n",
    "            yield feature_arrays\n",
    "\n",
    "    def collate_fn(self, padded_batch):\n",
    "        padded_batch = collate_feature_dict(padded_batch)\n",
    "        \n",
    "        target = padded_batch.payload[self.target_col_name]\n",
    "        time = padded_batch.payload['time']\n",
    "        del padded_batch.payload[self.target_col_name]\n",
    "        if self.target_dtype is not None:\n",
    "            target = target.to(dtype=self.target_dtype)\n",
    "\n",
    "        return padded_batch, target, time\n",
    "\n",
    "\n",
    "class SeqToTargetIterableDataset(SeqToTargetDataset, torch.utils.data.IterableDataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, dl, device='cuda:0'):\n",
    "    logits = []\n",
    "    model.to(device)\n",
    "    softmax = torch.nn.Softmax(dim=0) \n",
    "    for batch in tqdm.tqdm(dl, position=0, leave=True):\n",
    "        with torch.no_grad():\n",
    "            x, _, _ = batch\n",
    "            y_h, t_h = model(x.to(device))\n",
    "            logits.extend([y_h.cpu()])\n",
    "        \n",
    "    logits = softmax(torch.vstack(logits)[:, 1]).cpu()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "models = []\n",
    "predictions_5folds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, (train_index, test_index) in enumerate(skf.split(df_train.drop(columns=['target']), df_train['target'])):\n",
    "    train_, test_ = df_train.iloc[train_index], df_train.iloc[test_index]\n",
    "    \n",
    "    dataset_train = train_.to_dict(orient='records')\n",
    "    dataset_test = test_.to_dict(orient='records')\n",
    "    \n",
    "    sup_dataset = PtlsDataModule(\n",
    "        train_data=SeqToTargetDataset(\n",
    "            AugmentationDataset(\n",
    "                dataset_train,\n",
    "                f_augmentations=[\n",
    "                    DropoutTrx(0.1),\n",
    "                ],\n",
    "            ),\n",
    "            target_col_name='target',\n",
    "            target_dtype=torch.long,\n",
    "        ),\n",
    "        valid_data=SeqToTargetDataset(\n",
    "            dataset_test,\n",
    "            target_col_name='target',\n",
    "            target_dtype=torch.long,\n",
    "        ),\n",
    "        train_batch_size=256,\n",
    "        train_num_workers=8,\n",
    "        train_drop_last=True,\n",
    "\n",
    "        valid_batch_size=256,\n",
    "        valid_num_workers=8,\n",
    "        valid_drop_last=True\n",
    "    )\n",
    "    \n",
    "    seq_encoder.load_state_dict(torch.load('../models/coles-wtte-model1.pt'))\n",
    "\n",
    "    sup_module = SequenceToTarget(\n",
    "        seq_encoder=seq_encoder,\n",
    "        head=torch.nn.Sequential(\n",
    "            torch.nn.Linear(seq_encoder.embedding_size, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(512, 2),\n",
    "            torch.nn.LogSoftmax(dim=1),\n",
    "        ),\n",
    "        head_time=torch.nn.Sequential(\n",
    "            torch.nn.Linear(seq_encoder.embedding_size, 1),\n",
    "        ),\n",
    "        loss=torch.nn.NLLLoss(),\n",
    "        metric_list=torchmetrics.AUROC(num_classes=2),\n",
    "        optimizer_partial=partial(torch.optim.AdamW, lr=1e-3, weight_decay=0.0),\n",
    "        lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=1, gamma=0.2),\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=TensorBoardLogger('lightning_logs', name=f'CoLES-WTTE-supervised_{i}'),\n",
    "        max_epochs=1,\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        enable_progress_bar=True,\n",
    "        gradient_clip_algorithm='norm',\n",
    "        gradient_clip_val=0.18\n",
    "    )\n",
    "    \n",
    "    trainer.fit(sup_module, sup_dataset)\n",
    "    \n",
    "    torch.save(sup_module.state_dict(), f\"../models/sup_modules-wtte-kfold.{i}.pt\")\n",
    "    \n",
    "    predictions_test = test_[[\"user_id\"]].copy()\n",
    "    \n",
    "    dataset = SeqToTargetDataset(\n",
    "        data=dataset_test,\n",
    "        target_col_name='target',\n",
    "    )\n",
    "\n",
    "    dl = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        shuffle=False,\n",
    "        batch_size=512,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    \n",
    "    predictions_test[\"sp\"] = inference(sup_module, dl)\n",
    "    \n",
    "    predictions_5folds.append(predictions_test)\n",
    "    \n",
    "    print(12*\"-\")\n",
    "    print(\"AUROC; 5th fold:\", roc_auc_score(test_[\"target\"].values, predictions_test[\"sp\"]))\n",
    "    print(12*\"-\")\n",
    "    \n",
    "    models.append(deepcopy(sup_module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_5folds = pd.concat(predictions_5folds, axis=0)\n",
    "temp = predictions_5folds.merge(train_df[[\"user_id\", \"target\"]], on=\"user_id\")\n",
    "print(12*\"-\")\n",
    "print(\"AUROC; 5th fold:\", roc_auc_score(temp[\"target\"].values, temp[\"sp\"].values))\n",
    "print(12*\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = df_test.copy()\n",
    "dataset_test[[\"target\", \"time\"]] = None\n",
    "dataset_test = dataset_test.to_dict(orient='records')\n",
    "\n",
    "dataset = SeqToTargetDataset(\n",
    "    data=dataset_test,\n",
    "    target_col_name='target',\n",
    ")\n",
    "\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    shuffle=False,\n",
    "    batch_size=512,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "predictions_test = df_test[[\"user_id\"]].copy()\n",
    "\n",
    "for i in range(5):\n",
    "    predictions_test[f\"sp_{i}\"] = inference(models[i], dl)\n",
    "predictions_test[\"sp\"] = predictions_test.iloc[:, 1:].mean(axis=1)\n",
    "predictions_test\n",
    "print(12*\"-\")\n",
    "print(\"AUROC; 5th fold:\", roc_auc_score(df_test[\"target\"].values, predictions_test[\"sp\"].values))\n",
    "print(12*\"-\")\n",
    "train_test_predictions = pd.concat([predictions_5folds, predictions_test[[\"user_id\", \"sp\"]]], axis=0)\n",
    "#train_test_predictions.to_csv(\"sp-preds_train-test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submit = pd.read_csv(\"../data/sample_submit_naive.csv\")\n",
    "sbmt_df = pd.DataFrame(tr_dataset).drop('target', axis=1).merge(sample_submit[['user_id']], on='user_id')\n",
    "sbmt_df[[\"target\", \"time\"]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbmt_models = []\n",
    "for i, (train_index, test_index) in enumerate(skf.split(df.drop(columns=['target']), df[\"target\"])):\n",
    "    train_, test_ = df.iloc[train_index], df.iloc[test_index]\n",
    "    \n",
    "    dataset_train = train_.to_dict(orient='records')\n",
    "    dataset_test = test_.to_dict(orient='records')\n",
    "    \n",
    "    sup_dataset = PtlsDataModule(\n",
    "        train_data=SeqToTargetDataset(\n",
    "            AugmentationDataset(\n",
    "                dataset_train,\n",
    "                f_augmentations=[\n",
    "                    DropoutTrx(0.1),\n",
    "                ],\n",
    "            ),\n",
    "            target_col_name='target',\n",
    "            target_dtype=torch.long,\n",
    "        ),\n",
    "        valid_data=SeqToTargetDataset(\n",
    "            dataset_test,\n",
    "            target_col_name='target',\n",
    "            target_dtype=torch.long,\n",
    "        ),\n",
    "        train_batch_size=256,\n",
    "        train_num_workers=8,\n",
    "        train_drop_last=True,\n",
    "\n",
    "        valid_batch_size=256,\n",
    "        valid_num_workers=8,\n",
    "        valid_drop_last=True\n",
    "    )\n",
    "    \n",
    "    seq_encoder.load_state_dict(torch.load('../models/coles-wtte-model1.pt'))\n",
    "\n",
    "    sup_module = SequenceToTarget(\n",
    "        seq_encoder=seq_encoder,\n",
    "        head=torch.nn.Sequential(\n",
    "            torch.nn.Linear(seq_encoder.embedding_size, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(512, 2),\n",
    "            torch.nn.LogSoftmax(dim=1),\n",
    "        ),\n",
    "        head_time=torch.nn.Sequential(\n",
    "            torch.nn.Linear(seq_encoder.embedding_size, 1),\n",
    "        ),\n",
    "        loss=torch.nn.NLLLoss(),\n",
    "        metric_list=torchmetrics.AUROC(num_classes=2),\n",
    "        optimizer_partial=partial(torch.optim.AdamW, lr=4e-4, weight_decay=0.0),\n",
    "        lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=1, gamma=0.2),\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        logger=TensorBoardLogger('lightning_logs', name=f'CoLES-WTTE-supervised-sbmt_{i}'),\n",
    "        max_epochs=1,\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        enable_progress_bar=True,\n",
    "        gradient_clip_algorithm='norm',\n",
    "        gradient_clip_val=0.2\n",
    "    )\n",
    "    \n",
    "    trainer.fit(sup_module, sup_dataset)\n",
    "    \n",
    "    #torch.save(sup_module.state_dict(), f\"model/sup_modules-kfold/sbmt-model-0.{i}.pt\")\n",
    "    \n",
    "    dataset = SeqToTargetDataset(\n",
    "        data=dataset_test,\n",
    "        target_col_name='target',\n",
    "    )\n",
    "\n",
    "    dl = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "        shuffle=False,\n",
    "        batch_size=512,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    \n",
    "    predictions_test = inference(sup_module, dl)\n",
    "    \n",
    "    print(12*\"-\")\n",
    "    print(\"AUROC; 5th fold:\", roc_auc_score(test_[\"target\"].values, predictions_test))\n",
    "    print(12*\"-\")\n",
    "    \n",
    "    sbmt_models.append(deepcopy(sup_module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sbmt = sbmt_df.copy()\n",
    "dataset_sbmt[[\"target\", \"time\"]] = None\n",
    "dataset_sbmt = dataset_sbmt.to_dict(orient='records')\n",
    "\n",
    "dataset = SeqToTargetDataset(\n",
    "    data=dataset_sbmt,\n",
    "    target_col_name='target',\n",
    ")\n",
    "\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    shuffle=False,\n",
    "    batch_size=512,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "predictions_sbmt = sbmt_df[[\"user_id\"]].copy()\n",
    "\n",
    "for i in range(5):\n",
    "    predictions_sbmt[f\"sp_{i}\"] = inference(sbmt_models[i], dl)\n",
    "\n",
    "predictions_sbmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sbmt[\"sp\"] = predictions_sbmt.iloc[:, 1:].mean(axis=1)\n",
    "predictions_sbmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = pd.concat([predictions_sbmt[[\"user_id\", \"sp\"]], train_test_predictions])\n",
    "final_prediction.to_csv(\"../predicts/sup-wtte-preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESF",
   "language": "python",
   "name": "esf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
